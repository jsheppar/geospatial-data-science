{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5: Machine learning in Python\n",
    "\n",
    "**Objectives:**\n",
    "   * Engineer some features for better prediction of California house prices \n",
    "   * Train a machine learning model using <code>scikit-learn</code>\n",
    "   * Evaluate our machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "The most important part of data science is generating new features that have predictive power. We just used the default variables for predicting house prices in the lecture but there are other factors that may be useful.\n",
    "\n",
    "For example, we often have **geolocation data**, which could be very useful for house price prediction task. In this demo we will engineer some new features to improve the accuracy of our house price prediction model.\n",
    "\n",
    "As a recap, these were the mean-sqaured-errors from the lecture demo:\n",
    "\n",
    "* Multiple linear regression: $64,374\n",
    "\n",
    "\n",
    "* Decision Tree: $82,290\n",
    "\n",
    "\n",
    "* RandomForests: $60,264\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "# now you can import normally from model_selection\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('./data/seattle_house_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['long'], df['lat']))\n",
    "gdf = gdf.set_crs(4326, allow_override=True)\n",
    "\n",
    "# Reproject everything to UTM 10N (EPSG:32610)\n",
    "gdf_utm = gdf.to_crs('EPSG:32610')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price          1.000000\n",
       "sqft_living    0.702296\n",
       "bathrooms      0.524395\n",
       "bedrooms       0.315804\n",
       "lat            0.308082\n",
       "sqft_lot       0.090125\n",
       "yr_built       0.052453\n",
       "long           0.020092\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = gdf_utm.corr()\n",
    "\n",
    "# Display absolute house value correlations\n",
    "corr_matrix[\"price\"].abs().sort_values(ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "## Question 1 (10 points): \n",
    "\n",
    "To start, make a **new** `jupyter notebook` called `lab5_submission.ipynb` and work through the following tasks. \n",
    "\n",
    "The first task is answer the following questions using some of the methods we have covered in the lecture/demo. \n",
    "\n",
    "* How many houses are in this dataset?\n",
    "* How many **features** are there for predicting house price? \n",
    "* Are there any null values in this dataset?\n",
    "* Which three variables are best correlated with house price (include correlation coefficients)?\n",
    "* Which three variables are least correlated with house price (include correlation coefficients)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19451 houses in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Houses in dataset\n",
    "print(\"There are %i houses in the dataset.\" %len(gdf_utm['price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 features (or independent variables) available for predicting house princes.\n"
     ]
    }
   ],
   "source": [
    "# Features available in dataset\n",
    "print(\"There are %i features (or independent variables) available for predicting house princes.\" %(len(gdf_utm.columns)-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 null values in this dataset.\n"
     ]
    }
   ],
   "source": [
    "# Number of null values in dataset\n",
    "print(\"There are %i null values in this dataset.\" %(gdf_utm.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three best correlated features are sqft_living, bathrooms, and bedrooms, with correlations of 0.70, 0.52, 0.32, repsectively.\n"
     ]
    }
   ],
   "source": [
    "# Highest correlation\n",
    "var_high_corr = (corr_matrix[\"price\"].abs().sort_values(ascending= False)).index[1:4]\n",
    "high_corr_val = (corr_matrix[\"price\"].abs().sort_values(ascending= False)).values[1:4]\n",
    "\n",
    "print(\"The three best correlated features are %s, %s, and %s, with correlations of %.2f, %.2f, %.2f, repsectively.\" %(*var_high_corr, *high_corr_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three least correlated features are sqft_lot, yr_built, and long, with correlations of 0.09, 0.05, 0.02, repsectively.\n"
     ]
    }
   ],
   "source": [
    "# Lowest correlation\n",
    "var_high_corr = (corr_matrix[\"price\"].abs().sort_values(ascending= False)).index[-3:]\n",
    "high_corr_val = (corr_matrix[\"price\"].abs().sort_values(ascending= False)).values[-3:]\n",
    "\n",
    "print(\"The three least correlated features are %s, %s, and %s, with correlations of %.2f, %.2f, %.2f, repsectively.\" %(*var_high_corr, *high_corr_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******\n",
    "\n",
    "## Question 2 (30 points):\n",
    "\n",
    "* Produce a model to predict house prices. You are welcome to generate new features, scale the data, and split the data into training/testing (i.e. `train_test_split`) in any way you like. \n",
    "\n",
    "\n",
    "* Evaluate your model's accuracy by predicting a test dataset, for example:\n",
    "\n",
    "`predictions = forest_reg.predict(X_test)\n",
    "final_mse = mean_squared_error(y_test, predictions)\n",
    "final_rmse = np.sqrt(final_mse)`\n",
    "\n",
    "\n",
    "* Push your `lab5_submission.ipynb` to GitHub and submit a `.pdf` version to Canvas \n",
    "\n",
    "\n",
    "\n",
    "* On **Monday** the instructor and TA will provide an **unseen set of houses** which students will use to repeat their accuracy evaluation. The best models (i.e. lowest RMSE) will win prizes. \n",
    "\n",
    "\n",
    "* We will evaluate the models using a simple `mean-squared-error` as follows:\n",
    "\n",
    "`mse = mean_squared_error(y_test , predictions)\n",
    "rmse = np.sqrt(final_mse)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature list\n",
    "feature_list =  ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'yr_built', 'lat', 'long']\n",
    "\n",
    "# Define features and labels \n",
    "X = gdf_utm[feature_list]\n",
    "y = gdf_utm['price']\n",
    "\n",
    "# Standarize data\n",
    "scaler = StandardScaler()  \n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE_train: 58917.329253\n",
      "\n",
      "RMSE_test: 153340.054787\n",
      "\n",
      "train_score: 0.974027\n",
      "\n",
      "test_score: 0.834690\n",
      "\n",
      "oob_score: 0.816330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "forest_reg = RandomForestRegressor(n_jobs=-1,  oob_score=True, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "forest_reg.fit(X_train, y_train)\n",
    "\n",
    "def rmse(y_pred,y):\n",
    "    return np.sqrt(((y_pred-y)**2).mean())\n",
    "\n",
    "rmse_train = rmse(forest_reg.predict(X_train),y_train)\n",
    "rmse_test = rmse(forest_reg.predict(X_test),y_test)\n",
    "train_score = forest_reg.score(X_train,y_train)\n",
    "test_score = forest_reg.score(X_test,y_test)\n",
    "oob_score = forest_reg.oob_score_\n",
    "\n",
    "print(\"\"\"\n",
    "RMSE_train: %f\\n\n",
    "RMSE_test: %f\\n\n",
    "train_score: %f\\n\n",
    "test_score: %f\\n\n",
    "oob_score: %f\n",
    "\"\"\" %(rmse_train, rmse_test, train_score, test_score, oob_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper-paramterizing has been run, but is now commented out, so that the conversion to PDF doesn't take a long time. The found parameters can be found two cells after this text, in the input paramters for the forest_reg_hp model. The exception is max_depth, which has changed in the code, but I don't feel like running the hyper-parametrization again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 7,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 2,\n",
       " 'n_estimators': 170}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # hyper parametrize: n_estimators, min_samples_leaf, max_features\n",
    "# n_estimators = [int(x) for x in np.arange(start = 10, stop = 500, step = 10)]\n",
    "# max_features = [0.25, 0.5, 0.75, 1, 'auto', 'sqrt','log2']\n",
    "# min_samples_leaf = [1, 2, 3, 4, 5, 6, 7]\n",
    "# max_depth = [int(x) for x in np.arange(start = 1, stop = 300, step = 10)]\n",
    "# bootstrap = [True, False]\n",
    "# param_dict = {'n_estimators': n_estimators, 'max_features': max_features, 'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap, 'max_depth' : max_depth}\n",
    "\n",
    "# # First create the base model to tune\n",
    "# forest_reg_hp = RandomForestRegressor()\n",
    "# # Fit the random search model\n",
    "# forest_reg_random = HalvingGridSearchCV(estimator=forest_reg_hp, param_grid=param_dict, scoring='neg_root_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "# forest_reg_random.fit(X_train, y_train)\n",
    "# forest_reg_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE_train: 58388.492486\n",
      "\n",
      "RMSE_test: 153220.218147\n",
      "\n",
      "train_score: 0.974491\n",
      "\n",
      "test_score: 0.834949\n",
      "\n",
      "oob_score: 0.816007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "forest_reg_hp = RandomForestRegressor(n_estimators=170, max_depth=200, min_samples_leaf=1, n_jobs=-1, oob_score=True, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "forest_reg_hp.fit(X_train, y_train)\n",
    "\n",
    "rmse_train_hp = rmse(forest_reg_hp.predict(X_train),y_train)\n",
    "rmse_test_hp = rmse(forest_reg_hp.predict(X_test),y_test)\n",
    "train_score_hp = forest_reg_hp.score(X_train,y_train)\n",
    "test_score_hp = forest_reg_hp.score(X_test,y_test)\n",
    "oob_score_hp = forest_reg_hp.oob_score_\n",
    "\n",
    "print(\"\"\"\n",
    "RMSE_train: %f\\n\n",
    "RMSE_test: %f\\n\n",
    "train_score: %f\\n\n",
    "test_score: %f\\n\n",
    "oob_score: %f\n",
    "\"\"\" %(rmse_train_hp, rmse_test_hp, train_score_hp, test_score_hp, oob_score_hp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE_train dif (negative better): -528.836767\n",
      "RMSE_test dif (negative better): -119.836640\n",
      "\n",
      "train_score dif (positive better): 0.000464\n",
      "\n",
      "test_score dif (positive better): 0.000258\n",
      "\n",
      "oob_score dif (positive better): -0.000323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display effects of hyperfitting\n",
    "\n",
    "print(\"\"\"\n",
    "RMSE_train dif (negative better): %f\n",
    "RMSE_test dif (negative better): %f\\n\n",
    "train_score dif (positive better): %f\\n\n",
    "test_score dif (positive better): %f\\n\n",
    "oob_score dif (positive better): %f\n",
    "\"\"\" %(rmse_train_hp-rmse_train, rmse_test_hp-rmse_test, train_score_hp-train_score, test_score_hp-test_score, oob_score_hp-oob_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to hyper-parametrize my model rather than add new features. The process has technically worked, but only by a little. The RMSE for the test data came back $119.84 smaller than the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "\n",
    "## Task 1 (10 points):\n",
    "\n",
    "Submit a project idea to the `#final-projects` channel on **Slack**. See: https://github.com/JohnnyRyan1/geospatial-data-science/blob/master/labs/lab5/project_ideas_task.md for more details.\n",
    "\n",
    "\n",
    "## Remember to submit your answers to Questions 1 and 2 and complete Task 1 **by Friday 11:59pm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Jon Sheppard"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
